{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from os import chdir \n",
    "chdir('C:\\\\Users\\\\afeki\\\\Desktop\\\\Code\\\\BootStrapDesicionTree\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_boosting_trees.gradient_boosting_regressor import CartGradientBoostingRegressor, \\\n",
    "    CartGradientBoostingRegressorKfold\n",
    "from gradient_boosting_trees.gradient_boosting_abstract import GradientBoostingMachine\n",
    "from Tree.node import Leaf\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = 2\n",
    "A2 = 5\n",
    "SIGMNA = 3\n",
    "N_ROWS = 10**3\n",
    "VAL_RATIO = 0.15\n",
    "CATEGORICAL_DISTRIBUTION = 'uniform'\n",
    "CATEGORY_SIZE = 100\n",
    "CATEGORY_COLUMN_NAME = 'category'\n",
    "Y_COL_NAME = 'y'\n",
    "LEFT_GROUP = [i for i in range(CATEGORY_SIZE//2)]\n",
    "SEED = 5\n",
    "np.random.seed(SEED)\n",
    "CATEGORIES = np.arange(10, 200, 10)\n",
    "N_EXPERIMENTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 4\n",
    "N_ESTIMATORS = 100\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_val = {}\n",
    "results_df = pd.DataFrame(index = CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simulated data\n",
    "$$ y = a_1 \\cdot x_1 + a_2 \\cdot I(x_2 \\in LEFT\\_GROUP) + \\sigma $$\n",
    "\n",
    "$$ x1 - N(0,1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y(category_size):\n",
    "    X = pd.DataFrame()\n",
    "    X[CATEGORY_COLUMN_NAME] = np.random.randint(0, CATEGORY_SIZE, N_ROWS)\n",
    "    X['x1'] = np.random.randn(N_ROWS)\n",
    "    X[CATEGORY_COLUMN_NAME] = X[CATEGORY_COLUMN_NAME].astype('category')\n",
    "    sigma = SIGMNA*np.random.randn(N_ROWS)\n",
    "    y = A1*X['x1'] + A2*X[CATEGORY_COLUMN_NAME].isin(LEFT_GROUP) + sigma\n",
    "    return X,y\n",
    "\n",
    "def create_x(category_size):\n",
    "    X = pd.DataFrame()\n",
    "    X[CATEGORY_COLUMN_NAME] = np.random.randint(0, CATEGORY_SIZE, N_ROWS)\n",
    "    X['x1'] = np.random.randn(N_ROWS)\n",
    "    X[CATEGORY_COLUMN_NAME] = X[CATEGORY_COLUMN_NAME].astype('category')\n",
    "    sigma = SIGMNA*np.random.randn(N_ROWS)\n",
    "    X[Y_COL_NAME] = A1*X['x1'] + A2*X[CATEGORY_COLUMN_NAME].isin(LEFT_GROUP) + sigma\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.core import Booster\n",
    "def compute_ntrees_nleaves(gbm):\n",
    "    total_number_of_trees = 0\n",
    "    total_number_of_leaves = 0\n",
    "    if isinstance(gbm,GradientBoostingMachine):\n",
    "        for tree in gbm.trees:\n",
    "            if not isinstance(tree.root, Leaf):\n",
    "                total_number_of_trees += 1\n",
    "                total_number_of_leaves += tree.n_leaves\n",
    "    elif isinstance(gbm,GradientBoostingClassifier) or isinstance(gbm,GradientBoostingRegressor):\n",
    "        total_number_of_trees = gbm.n_estimators_\n",
    "        for tree in gbm.estimators_:\n",
    "            total_number_of_leaves += tree[0].get_n_leaves()\n",
    "    elif isinstance(gbm, Booster):\n",
    "        df = gbm.trees_to_dataframe()\n",
    "        total_number_of_leaves = df[df['Feature'] == 'Leaf'].shape[0]\n",
    "        total_number_of_trees = df[df['Feature'] == 'Leaf']['Tree'].max()\n",
    "\n",
    "    print(F'number of trees is {total_number_of_trees}')\n",
    "    print(F'number of leaves is {total_number_of_leaves}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# Pandas one hot encoder\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.unique_values_for_col = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            self.unique_values_for_col[col] = X[col].unique().tolist()\n",
    "\n",
    "    def transform(self, X):\n",
    "        dataframes = []\n",
    "        for col in X.columns:\n",
    "            type = CategoricalDtype(categories=self.unique_values_for_col[col])\n",
    "            dataframes.append(pd.get_dummies(pd.Series(X[col], dtype=type), prefix=col))\n",
    "        return pd.concat(dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR IMPLEMENTATION\n",
    "## KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [06:55<00:00, 21.86s/it]\n",
      " 42%|████▏     | 8/19 [02:30<03:22, 18.42s/it]"
     ]
    }
   ],
   "source": [
    "exp_name = 'Ours_Kfold'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X = create_x(CATEGORY_SIZE)\n",
    "        X, X_val = train_test_split(X, test_size=VAL_RATIO, random_state=42)\n",
    "        y_val = X_val[Y_COL_NAME]\n",
    "        X_val =  X_val.drop(columns = [Y_COL_NAME])\n",
    "        reg = CartGradientBoostingRegressorKfold(Y_COL_NAME, max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE)\n",
    "        reg.fit(X)\n",
    "        fi = pd.Series(reg.compute_feature_importance()).sort_index()\n",
    "        fi/= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -reg.predict(X_val))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Tree.tree_visualizer import TreeVisualizer\n",
    "# tree_vis = TreeVisualizer()\n",
    "# tree_vis.plot(kfold_gbm.trees[0].root)\n",
    "\n",
    "# compute_ntrees_nleaves(kfold_gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla gradient boosting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Ours_Vanilla'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X = create_x(CATEGORY_SIZE)\n",
    "        X, X_val = train_test_split(X, test_size=VAL_RATIO, random_state=42)\n",
    "        y_val = X_val[Y_COL_NAME]\n",
    "        X_val =  X_val.drop(columns = [Y_COL_NAME])\n",
    "        vanilla_gbm = CartGradientBoostingRegressor(Y_COL_NAME, max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE)\n",
    "        reg.fit(X)\n",
    "        fi = pd.Series(reg.compute_feature_importance()).sort_index()\n",
    "        fi/= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -reg.predict(X_val))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_ntrees_nleaves(vanilla_gbm)\n",
    "# tree_vis = TreeVisualizer()\n",
    "# tree_vis.plot(vanilla_gbm.trees[0].root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours Mean Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_imputing_x_x_val(x, y,x_val):\n",
    "    temp_x = x.copy()\n",
    "    temp_x[Y_COL_NAME] = y\n",
    "    category_to_mean = temp_x.groupby(CATEGORY_COLUMN_NAME)[Y_COL_NAME].mean().to_dict()\n",
    "    temp_x[CATEGORY_COLUMN_NAME] = temp_x[CATEGORY_COLUMN_NAME].map(category_to_mean)\n",
    "    temp_x = temp_x.drop(columns = [Y_COL_NAME])\n",
    "    temp_x[CATEGORY_COLUMN_NAME] = temp_x[CATEGORY_COLUMN_NAME].astype('float')\n",
    "    x_val[CATEGORY_COLUMN_NAME] = x_val[CATEGORY_COLUMN_NAME].map(category_to_mean)\n",
    "    x_val[CATEGORY_COLUMN_NAME] = x_val[CATEGORY_COLUMN_NAME].astype('float')\n",
    "    x_val[CATEGORY_COLUMN_NAME] = x_val[CATEGORY_COLUMN_NAME].fillna(x_val[CATEGORY_COLUMN_NAME].mean())\n",
    "\n",
    "    return temp_x , x_val\n",
    "\n",
    "exp_name = 'Ours_MeanImputing'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        X, X_val = create_mean_imputing_x_x_val(X, y,X_val)\n",
    "        X[Y_COL_NAME] = y\n",
    "        reg = CartGradientBoostingRegressor(Y_COL_NAME, max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE)\n",
    "        reg.fit(X)\n",
    "        fi = pd.Series(reg.compute_feature_importance()).sort_index()\n",
    "        fi/= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -reg.predict(X_val))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours mean imputing with kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Ours_MeanImputingKfold'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        X, X_val = create_mean_imputing_x_x_val(X, y,X_val)\n",
    "        X[Y_COL_NAME] = y\n",
    "        reg = CartGradientBoostingRegressorKfold(Y_COL_NAME, max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE)\n",
    "        reg.fit(X)\n",
    "        fi = pd.Series(reg.compute_feature_importance()).sort_index()\n",
    "        fi/= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -reg.predict(X_val))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN - One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_x_x_val(x, x_val):\n",
    "    one_hot = OneHotEncoder()\n",
    "    one_hot.fit(x[CATEGORY_COLUMN_NAME].to_frame())\n",
    "    x_one_hot = one_hot.transform(x[CATEGORY_COLUMN_NAME].to_frame())\n",
    "    x_one_hot['x1'] = X['x1']\n",
    "    x_one_hot_val = one_hot.transform(x_val[CATEGORY_COLUMN_NAME].to_frame())\n",
    "    x_one_hot_val['x1'] = X_val['x1']\n",
    "    return x_one_hot, x_one_hot_val\n",
    "    \n",
    "exp_name = 'Sklearn_OneHot'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        X, X_val = create_one_hot_x_x_val(X, X_val)\n",
    "        reg = GradientBoostingRegressor(n_estimators=  N_ESTIMATORS, max_depth = MAX_DEPTH - 1,learning_rate = LEARNING_RATE)\n",
    "        reg.fit(X, y)\n",
    "        fi = fi = pd.Series(reg.feature_importances_, index = X.columns)\n",
    "        fi/= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -reg.predict(X_val))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN - Mean Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Sklearn_MeanImputing'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        X, X_val = create_mean_imputing_x_x_val(X, y,X_val)\n",
    "        reg = GradientBoostingRegressor(n_estimators=  N_ESTIMATORS, max_depth = MAX_DEPTH - 1,learning_rate = LEARNING_RATE)\n",
    "        reg.fit(X, y)\n",
    "        fi = fi = pd.Series(reg.feature_importances_, index = X.columns)\n",
    "        fi/= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -reg.predict(X_val))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST - One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'XGBOOST_OneHot'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        X, X_val = create_one_hot_x_x_val(X, X_val)\n",
    "        dtrain = xgb.DMatrix(X, label=y)\n",
    "        param = {'max_depth': MAX_DEPTH - 1, 'eta': LEARNING_RATE, 'objective': 'reg:linear'}\n",
    "        num_round = N_ESTIMATORS\n",
    "        bst = xgb.train(param, dtrain, num_round)\n",
    "        fi = pd.Series(bst.get_score(importance_type='gain'))\n",
    "        fi /= fi.sum()\n",
    "        dval = xgb.DMatrix(X_val)\n",
    "        temp_results.append(np.mean(np.square(y_val -bst.predict(dval))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST - Mean Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'XGBOOST_MeanImputing'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        X, X_val = create_mean_imputing_x_x_val(X, y,X_val)\n",
    "        dtrain = xgb.DMatrix(X, label=y)\n",
    "        param = {'max_depth': MAX_DEPTH - 1, 'eta': LEARNING_RATE, 'objective': 'reg:linear'}\n",
    "        num_round = N_ESTIMATORS\n",
    "        bst = xgb.train(param, dtrain, num_round)\n",
    "        fi = pd.Series(bst.get_score(importance_type='gain'))\n",
    "        fi /= fi.sum()\n",
    "        dval = xgb.DMatrix(X_val)\n",
    "        temp_results.append(np.mean(np.square(y_val -bst.predict(dval))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'CATBOOST_MeanImputing'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        X, X_val = create_mean_imputing_x_x_val(X, y,X_val)\n",
    "        train_pool = Pool(X, y)\n",
    "        model = CatBoostRegressor(iterations=N_ESTIMATORS, \n",
    "                                  depth=MAX_DEPTH, \n",
    "                                  learning_rate=LEARNING_RATE, \n",
    "                                  loss_function='RMSE',logging_level = 'Silent')\n",
    "        model.fit(train_pool)\n",
    "        val_pool = Pool(X_val)\n",
    "#         fi = pd.Series(bst.get_score(importance_type='gain'))\n",
    "#         fi /= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -model.predict(val_pool))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'CATBOOST_Vanilla'\n",
    "mean_results = []\n",
    "for exp in range(N_EXPERIMENTS):\n",
    "    temp_results = []\n",
    "    for category_size in tqdm(CATEGORIES, total=len(CATEGORIES)):\n",
    "        np.random.seed(exp)\n",
    "        X, y = create_x_y(CATEGORY_SIZE)\n",
    "        X, X_val, y, y_val = train_test_split(X,y, test_size=VAL_RATIO, random_state=42)\n",
    "        train_pool = Pool(X, y, cat_features=[0])\n",
    "        model = CatBoostRegressor(iterations=N_ESTIMATORS, \n",
    "                                  depth=MAX_DEPTH, \n",
    "                                  learning_rate=LEARNING_RATE, \n",
    "                                  loss_function='RMSE',logging_level = 'Silent')\n",
    "        model.fit(train_pool)\n",
    "        val_pool = Pool(X_val, cat_features=[0])\n",
    "#         fi = pd.Series(bst.get_score(importance_type='gain'))\n",
    "#         fi /= fi.sum()\n",
    "        temp_results.append(np.mean(np.square(y_val -model.predict(val_pool))))\n",
    "    mean_results.append(np.mean(np.array(temp_results)))\n",
    "results_df[exp_name] = mean_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(15,8))\n",
    "# chart = sns.barplot(x=\"index\", y= 0, data=pd.Series(mse_val).sort_values().to_frame().reset_index())\n",
    "# chart.set_xticklabels(chart.get_xticklabels(), rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "ax = results_df.plot()\n",
    "ax.set_xlabel(\"Number of categories\")\n",
    "ax.set_ylabel(\"Feature importance of a random vector\")\n",
    "ax.set_title(\"Feature importance of a random vector with K categories as a function of K\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
