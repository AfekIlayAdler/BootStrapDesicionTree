{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from os import chdir\n",
    "chdir(\"C:\\\\Users\\\\afeki\\\\OneDrive\\\\Desktop\\\\code\\\\BootStrapDesicionTree\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_boosting_trees.gradient_boosting_regressor import CartGradientBoostingRegressor, \\\n",
    "    CartGradientBoostingRegressorKfold\n",
    "from gradient_boosting_trees.gradient_boosting_abstract import GradientBoostingMachine\n",
    "from Tree.node import Leaf\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simulated data\n",
    "$$ y = a_1 \\cdot x_1 + a_2 \\cdot I(x_2 \\in LEFT\\_GROUP) + \\sigma $$\n",
    "\n",
    "$$ x1 - N(0,1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(model_name,model,X,y, transform_x = True, categorical_features = None):\n",
    "    if transform_x:\n",
    "        temp_x = X.copy()\n",
    "        if model_name == 'xgboost':\n",
    "            temp_x = xgb.DMatrix(temp_x)\n",
    "        elif model_name == 'catboost':\n",
    "            if categorical_features:\n",
    "                temp_x = Pool(temp_x , cat_features = categorical_features)\n",
    "            else:\n",
    "                temp_x = Pool(temp_x)\n",
    "        return np.mean(np.square(y - model.predict(temp_x)))\n",
    "    return np.mean(np.square(y - model.predict(X)))\n",
    "    \n",
    "    \n",
    "cat_features=[0]\n",
    "\n",
    "def permutation_feature_importance(model_name,model,X,y, categorical_features = None):\n",
    "    results = {}\n",
    "    mse = compute_mse(model_name, model,X,y, categorical_features = categorical_features)\n",
    "    for col in X.columns:\n",
    "        temp_x = X.copy()\n",
    "        temp_x[col] = np.random.permutation(temp_x[col])\n",
    "        if model_name == 'xgboost':\n",
    "            temp_x = xgb.DMatrix(temp_x)\n",
    "        elif model_name == 'catboost':\n",
    "            if categorical_features:\n",
    "                temp_x = Pool(temp_x , cat_features = categorical_features)\n",
    "            else:\n",
    "                temp_x = Pool(temp_x)\n",
    "        new_mse = compute_mse(model_name, model,temp_x,y, transform_x = False, categorical_features = categorical_features)\n",
    "        results[col] = new_mse - mse\n",
    "    results = pd.Series(results)\n",
    "    return results/results.sum()\n",
    "\n",
    "def get_x1_shap_value(model, x, columns = None):\n",
    "    if columns is None:\n",
    "        columns = x.columns\n",
    "    abs_shap_values = pd.DataFrame(shap.TreeExplainer(model, feature_perturbation = \"tree_path_dependent\").shap_values(x), columns = columns).apply(np.abs)\n",
    "    return (abs_shap_values.mean()/abs_shap_values.mean().sum())['x1']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import all_experiments, create_x_y, n_experiments, get_fitted_model, create_one_hot_x_x_val, create_mean_imputing_x_x_val\n",
    "from config import RESULTS_DIR,N_EXPERIMENTS,N_ROWS, SIGMA, CATEGORY_COLUMN_NAME, Y_COL_NAME, MAX_DEPTH, LEARNING_RATE,VAL_RATIO, N_ESTIMATORS,MODELS_DIR, N_EXPERIMENTS, CATEGORIES, A_VALUES\n",
    "EXP_NAME = F\"interaction_{MAX_DEPTH}_nestimators_{N_ESTIMATORS}_learning_rate_{LEARNING_RATE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4830/4830 [00:00<00:00, 67263.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ours_Kfold</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.101258</td>\n",
       "      <td>-0.042087</td>\n",
       "      <td>0.017631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ours_CartVanilla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.929568</td>\n",
       "      <td>0.491750</td>\n",
       "      <td>0.937485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ours_Kfold</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.171260</td>\n",
       "      <td>-0.175874</td>\n",
       "      <td>0.069948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ours_CartVanilla</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.937554</td>\n",
       "      <td>-0.098351</td>\n",
       "      <td>3.825850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ours_Kfold</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.178514</td>\n",
       "      <td>-0.049647</td>\n",
       "      <td>26.293417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 0    1     2         3         4          5\n",
       "0           0        ours_Kfold  2.0  0.00  0.101258 -0.042087   0.017631\n",
       "0           0  ours_CartVanilla  2.0  0.00  0.929568  0.491750   0.937485\n",
       "0           0        ours_Kfold  2.0  0.05  0.171260 -0.175874   0.069948\n",
       "0           0  ours_CartVanilla  2.0  0.05  0.937554 -0.098351   3.825850\n",
       "0           0        ours_Kfold  2.0  0.10  0.178514 -0.049647  26.293417"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chdir(\"C:\\\\Users\\\\afeki\\\\OneDrive\\\\Desktop\\\\code\\\\BootStrapDesicionTree\\\\experiments\\\\interaction\\\\\")\n",
    "from glob import glob\n",
    "all_paths = [] \n",
    "for exp_number, category_size, a in tqdm(all_experiments(), total=n_experiments):\n",
    "    np.random.seed(exp_number)\n",
    "    for predictor_name, predictor in {'Kfold': CartGradientBoostingRegressorKfold,\n",
    "                                      'CartVanilla': CartGradientBoostingRegressor}.items():\n",
    "        exp_name = F\"{EXP_NAME}__{predictor_name}_exp_{exp_number}_category_size_{category_size}_a_{a}\"\n",
    "        exp_results_path = RESULTS_DIR / F\"{exp_name}.csv\"\n",
    "        all_paths.append(exp_results_path)\n",
    "our_df = pd.concat([pd.read_csv(path) for path in all_paths])\n",
    "# our_df = our_df[our_df.columns]\n",
    "# our_df.columns = ['experiment','k','a','gain','permutation_train','permutation_test']\n",
    "# our_df = our_df.groupby(['experiment','k','a']).mean().reset_index()\n",
    "# our_df['shap_train'] = np.nan\n",
    "# our_df['shap_test'] = np.nan\n",
    "our_df.head()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4830/4830 [00:06<00:00, 712.50it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-790733103d1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msklearn_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'gain'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'permutation_train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'permutation_test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shap_train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'shap_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msklearn_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0msklearn_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0msklearn_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2938\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3000\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3612\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3613\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreindexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindexer\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   3602\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3603\u001b[0m                         \u001b[1;31m# duplicate axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3604\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3606\u001b[0m                     \u001b[1;31m# other\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindexer\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   3597\u001b[0m                 \u001b[1;31m# GH 4107\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3598\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3599\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3600\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3601\u001b[0m                     \u001b[1;31m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, index, **kwargs)\u001b[0m\n\u001b[0;32m   4028\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4029\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4030\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4032\u001b[0m     def drop(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4542\u001b[0m         \u001b[1;31m# perform the reindex on the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4543\u001b[0m         return self._reindex_axes(\n\u001b[1;32m-> 4544\u001b[1;33m             \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4545\u001b[0m         ).__finalize__(self)\n\u001b[0;32m   4546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   4565\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4566\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4567\u001b[1;33m                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4568\u001b[0m             )\n\u001b[0;32m   4569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   4611\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4612\u001b[0m                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4613\u001b[1;33m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4614\u001b[0m             )\n\u001b[0;32m   4615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_can_reindex\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   3097\u001b[0m         \u001b[1;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3098\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3099\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None \n",
    "all_paths = []\n",
    "model_type= 'sklearn'\n",
    "sklearn_df = pd.DataFrame(columns = ['experiment','k','a'])\n",
    "'experiment','k','a'\n",
    "i = 0\n",
    "for exp_number, category_size, a in tqdm(all_experiments(), total=n_experiments):\n",
    "    for predictor_name, _ in {'one_hot': GradientBoostingRegressor,\n",
    "                                      'mean_imputing': GradientBoostingRegressor}.items():\n",
    "        exp_name = F\"{EXP_NAME}_{model_type}_{predictor_name}_exp_{exp_number}_category_size_{category_size}_a_{a}\"\n",
    "        exp_results_path = RESULTS_DIR / F\"{exp_name}.csv\"\n",
    "        all_paths.append(exp_results_path)\n",
    "        sklearn_df.loc[i,:] = [F\"{model_type}_{predictor_name}\", category_size, a]\n",
    "        i+=1\n",
    "        \n",
    "sklearn_results = pd.concat(pd.read_csv(path) for path in all_paths if path.exists())\n",
    "sklearn_results = sklearn_results[sklearn_results.columns[1:]]\n",
    "sklearn_results.columns = ['gain','permutation_train','permutation_test', 'shap_train','shap_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type= 'xgboost'\n",
    "for exp_number, category_size, a in tqdm(all_experiments(), total=n_experiments):\n",
    "    np.random.seed(exp_number)\n",
    "    for predictor_name, _ in {'one_hot': GradientBoostingRegressor,\n",
    "                                      'mean_imputing': GradientBoostingRegressor}.items():\n",
    "        X, y = create_x_y(category_size, float(a))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_RATIO, random_state=42)\n",
    "        if predictor_name == 'one_hot':\n",
    "            X_train, X_test = create_one_hot_x_x_val(X_train, X_test)\n",
    "        elif predictor_name == 'mean_imputing':\n",
    "            X_train, X_test = create_mean_imputing_x_x_val(X_train,y_train, X_test)\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        param = {'max_depth': MAX_DEPTH - 1, 'eta': LEARNING_RATE, 'objective': 'reg:squarederror'}\n",
    "        bst = xgb.train(param, dtrain, N_ESTIMATORS)\n",
    "        fi_gain = pd.Series(bst.get_score(importance_type='gain'))\n",
    "        fi_gain /= fi_gain.sum()\n",
    "\n",
    "        fi_permutation_train = permutation_feature_importance(model_type,bst,X_train,y_train)\n",
    "        fi_permutation_test = permutation_feature_importance(model_type,bst, X_test, y_test)\n",
    "        temp_results = np.array([fi_gain['x1'],fi_permutation_train['x1'], fi_permutation_test['x1'],  get_x1_shap_value(bst, X_train),get_x1_shap_value(bst, X_test)])\n",
    "        results.setdefault((F\"{model_type}_{predictor_name}\", category_size, a),np.zeros(len(temp_results)))\n",
    "        results[(F\"{model_type}_{predictor_name}\", category_size, a)] += temp_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type= 'catboost'\n",
    "for exp_number, category_size, a in tqdm(all_experiments(), total=n_experiments):\n",
    "    np.random.seed(exp_number)\n",
    "    for predictor_name in [\"vanilla\",\"mean_imputing\"]:\n",
    "        X, y = create_x_y(category_size, float(a))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_RATIO, random_state=42)\n",
    "        if predictor_name == 'mean_imputing':\n",
    "            X_train, X_test = create_mean_imputing_x_x_val(X_train,y_train, X_test)\n",
    "            train_pool = Pool(X_train, y_train)\n",
    "            val_pool = Pool(X_test,y_test)\n",
    "        else:\n",
    "            train_pool = Pool(X_train, y_train, cat_features=[0])\n",
    "            val_pool = Pool(X_test,y_test, cat_features=[0])\n",
    "    \n",
    "        model = CatBoostRegressor(iterations=N_ESTIMATORS, \n",
    "                                  depth=MAX_DEPTH, \n",
    "                                  learning_rate=LEARNING_RATE, \n",
    "                                  loss_function='RMSE',logging_level = 'Silent')\n",
    "        model.fit(train_pool)\n",
    "        \n",
    "        fi_gain = pd.Series(model.feature_importances_, index = model.feature_names_)\n",
    "        fi_gain /= fi_gain.sum()\n",
    "        cat_feature = [0] if predictor_name == 'vanilla' else None\n",
    "        fi_permutation_train = permutation_feature_importance(model_type,model,X_train,y_train,  categorical_features = cat_feature)\n",
    "        fi_permutation_test = permutation_feature_importance(model_type,model, X_test, y_test, categorical_features = cat_feature)\n",
    "        temp_results = np.array([fi_gain['x1'], fi_permutation_train['x1'], fi_permutation_test['x1'],  get_x1_shap_value(model, train_pool, columns = X_train.columns),get_x1_shap_value(model, val_pool, columns = X_test.columns)])\n",
    "        results.setdefault((F\"{model_type}_{predictor_name}\", category_size, a),np.zeros(len(temp_results)))\n",
    "        results[(F\"{model_type}_{predictor_name}\", category_size, a)] += temp_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import pickle\n",
    "\n",
    "for key in results.keys():\n",
    "    results[key] = results[key]/N_EXPERIMENTS\n",
    "    \n",
    "# chdir(\"C:\\\\Users\\\\afeki\\\\OneDrive\\\\Desktop\\\\code\\\\BootStrapDesicionTree\\\\experiments\\\\interaction\\\\\")\n",
    "# with open('results/experiments_results/detect_uninformative_feature_results.pkl', 'wb') as output:\n",
    "#     pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments = list(set(exp[0] for exp in results.keys()))\n",
    "# categories = sorted(list(set(exp[1] for exp in results.keys())))\n",
    "# a_s = sorted(list(set(exp[2] for exp in results.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi_to_location = {'gain' :0, 'permutation_train': 1, 'permutation_test':2}\n",
    "# fi = 'gain'\n",
    "# a = 100\n",
    "# df = pd.DataFrame()\n",
    "# for exp in experiments:\n",
    "#     for cat in categories:   \n",
    "#         df.loc[exp,cat] = results[(exp,cat, a)][fi_to_location[fi]]\n",
    "        \n",
    "# plt.rcParams['figure.figsize'] = [20, 10]\n",
    "# ax = df.T.plot()\n",
    "# ax.set_xlabel(\"Number of categories\")\n",
    "# ax.set_ylabel(\"Feature importance of a random vector\")\n",
    "# ax.set_title(\"Feature importance of a random vector with K categories as a function of K\")\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi_to_location = {'gain' :0, 'permutation_train': 1, 'permutation_test':2}\n",
    "# fi = 'gain'\n",
    "# a = 100 # [2, 10, 50, 100, 200]\n",
    "\n",
    "# def plot_feature_importance(method, a1):\n",
    "#     df = pd.DataFrame()\n",
    "#     for exp in experiments:\n",
    "#         for cat in categories:   \n",
    "#             df.loc[exp,cat] = results[(exp,cat, a1)][fi_to_location[method]]\n",
    "\n",
    "#     plt.rcParams['figure.figsize'] = [20, 10]\n",
    "#     ax = df.T.plot()\n",
    "#     ax.set_xlabel(\"Number of categories\")\n",
    "#     ax.set_ylabel(\"Feature importance of the category vaector\")\n",
    "#     ax.set_title(\"Feature importance of the category vaector with K categories as a function of K\")\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
